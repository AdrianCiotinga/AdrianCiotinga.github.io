---
layout: post
title:  "The Power of Explainability: How We Aim To Build Trust In Computer Vision Models Using Saliency Maps"
date:   2023-08-31
categories: blog post
---
#### Prerequisites: basic understanding of what saliency maps are, some understanding of deep neural networks (particularly computer vision), and having read the paper on RISE (linked at the end) is a plus.

In case you've been living under a rock, computer vision is rapidly being integrated into many technologies we use every day. From its uses in autonomous driving systems to airport security to medical imagery processing, we are beginning to place more trust in the decisions made by computer vision systems. Furthermore, as the state-of-the-art progresses, machines that can "see" will be applied to continuously more difficult and higher-stakes tasks while simultaneously receiving less human oversight. However, we shouldn't conflate our desire for technological progress in applying computer vision systems with the need to disregard the trustworthiness of such a system; our efforts in understanding how a given computer vision system works should scale directly with the potential for said system to cause harm in the case of a failure.
## RISE, D-RISE, and MC-RISE
There are many techniques in the literature that generate saliency maps for a given input image and model pipeline. Commonly, these techniques are separated out into white-box and black-box categories based on the presumed knowledge of the explanation method - although this line can occasionally get blurry, such as deciding whether using pre-NMS boxes to compute a class probability vector for object detection systems or modifying an image after it has gone through preprocessing is still black-box (for what it's worth, I believe the former *can* be black-box but the latter is not). In particular, this blog post focuses on the Randomized Input Sampling for Explanation (RISE) family of saliency map generation techniques, which includes RISE, D-RISE (RISE for object detectors), and MC-RISE (multi-colored RISE). These techniques are all black-box and model-agnostic, generating saliency maps for a particular image given access to only the inputs and outputs of a model.

At their core, RISE-based techniques utilize masking to assign an "importance" value to each pixel in an image with respect to a given model output. Masks that mask out important pixel information for a given image will naturally cause the output of a model to change drastically, while masks that mask out unimportant pixel information will cause the output of a model to hardly change at all. Using a similarity metric, each mask is weighted by the similarity of its masked image's output to the unmasked image's output, and a weighted sum of the masks results in the saliency map.
![Image](https://camo.githubusercontent.com/a41672d5047e7c371e0854bd23b5cab5487a7a158aa41bd61470d9220dab62c6/68747470733a2f2f65636c697175652e6769746875622e696f2f7265702d696d67732f524953452f726973652d6f766572766965772e706e67 "Figure 1")

Figure 1: the process by which RISE-based techniques generate saliency maps

## Assumptions Made by RISE-based Techniques
In practice, the high-dimensionality of the pixel representation of images makes it impossible to mask individual pixels, so RISE-based techniques rely on a couple of assumptions about the workings of computer vision pipelines to generate saliency maps. The first of these assumptions is that pixels in close proximity of each other are similarly salient. This is a generally sound assumption, as the convolutional neural network architecture used by most computer vision systems - and even the patch representation used in vision transformers (although this could be a blog post on its own!) - has receptive fields before the fully-connected layers that ensure that local pixel values are the only things producing local features, and thus are likely similarly salient. Using this assumption, binary masks are generated at a downsampled resolution (often 8x8, 12x12, 16x16, 32x32, etc.), upsampled bilinearly, and randomly cropped to size. With a high enough number of random masks (I got good results with 2000 masks), the expected value of a given pixel across all masks tends towards the expected value of any pixel in any mask - i.e., no outlier pixels masked more or less often than the rest.

The second assumption made by RISE-based techniques is that pixel information can be masked out as shown above without adding additional information to an image. This assumption is less sound, as simply setting a pixel RGB value to `(0, 0, 0)` - or any value to take as "masked off" for that matter - does not give the pixel "zero" information. To give an analogy to Python programming, setting a variable by running `i = 0` is very different than setting a variable by running `i = None`, since the variable `i` in the first case holds a value while it does not in the second case. While masking off a pixel does overwrite the information in said pixel, the masked pixel itself still contains information! In the case of convolutional neural networks, there is no concept of a zero embedding like there is in something like a transformer architecture since raw pixel values are fed directly into the network, which is what led me to question the assumption that pixel information can be masked out of an image to begin with.

![Image](https://external-preview.redd.it/qvBxTrAYatMV_TMQcOXkRpeH7XMQq7gS2Dvo6bkrMl0.jpg?width=640&crop=smart&auto=webp&s=8cfc67807eeaf2221aa20feee598107d999de800 "Figure 2")

Figure 2: Another analogy relating to the use of a "masked" RGB value to remove pixel information. It is not possible to use `null` or `undefined` as pixel values. 

## So... Are RISE-based Saliency Map Generation Techniques Broken?
### No! 
Or at least not totally. There is still a lot we can learn from saliency maps generated using such techniques, such as whether a model is suitable to generalize outside of its training domain (search up "wolf-husky classifier" if you're not familiar with the example, it's a great - albeit slightly outdated - read). However, I felt that it was important to point out how making general assumptions about how computer vision models work can bring into question the integrity of a black-box explanation technique. It is possible to manipulate the saliency maps generated by GradCAM - a white-box saliency map generation technique used on convolutional neural networks - by adversarially manipulating a model's convolution layers. That is, understanding how GradCAM explains a model allows for the model to be changed to make it unexplainable. To give a little foreshadowing about future blog posts, could it be possible to take advantage of an assumption made by RISE-based saliency map generation techniques to craft a model unexplainable by even a state-of-the-art, black-box technique?
